{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolution_Neural_Network_Baseline_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aOlpMR0PGO1VPB2wGTLjtSi_wIbgXF6f",
      "authorship_tag": "ABX9TyMg0Lj5xFvvWUgToF1Lm7Q4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YagyanshB/SemEval-Task6-CS408/blob/main/Convolution_Neural_Network_Baseline_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMYC7oefV5-R"
      },
      "source": [
        "# Implementation of Convolutional Neural Network:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E3dnfj2GsShf",
        "outputId": "a7ee1451-8580-433c-9c32-4d4dc93e8933"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwoWIMvFsVWj"
      },
      "source": [
        "# IMPORT MODULES\n",
        "import numpy as np \n",
        "import re\n",
        "import pandas as pd \n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding \n",
        "from keras.layers import concatenate \n",
        "from keras.layers import Input \n",
        "from keras.layers import SpatialDropout1D \n",
        "from keras.layers import Dropout \n",
        "from keras.layers import GRU \n",
        "from keras.layers import Bidirectional \n",
        "from keras.layers import Flatten\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaKN-7LwsxAf"
      },
      "source": [
        "# INITIALIZATION\n",
        "\n",
        "train_file    = 'drive/My Drive/olid-training-v1.0.tsv' \n",
        "\n",
        "test_file_a   = 'drive/My Drive/testset-levela.tsv' \n",
        "test_file_b   = 'drive/My Drive/testset-levelb.tsv' # set corresponding path for the test dataset file\n",
        "test_file_c   = 'drive/My Drive/testset-levelc.tsv' # set corresponding path for the test dataset file\n",
        "\n",
        "test_labels_a = 'drive/My Drive/labels-levela.csv' \n",
        "test_labels_b = 'drive/My Drive/labels-levelb.csv' # set corresponding path for the labels of the test dataset file\n",
        "test_labels_c = 'drive/My Drive/labels-levelc.csv' # set corresponding path for the labels of the test dataset file\n",
        "\n",
        "\n",
        "maxlen = 200\n",
        "max_fatures = 10000\n",
        "\n",
        "# GloVe Pretrained Embeddings\n",
        "\n",
        "\n",
        "embedding_dim = 200\n",
        "\n",
        "models_a = list()\n",
        "models_b = list()\n",
        "models_c = list()\n",
        "ensemble_models_a = list()\n",
        "ensemble_models_b = list()\n",
        "ensemble_models_c = list()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGkC1gVVtFM6"
      },
      "source": [
        "# UTIL FUNCTIONS\n",
        "\n",
        "def replace_hashtag(tweet):\n",
        "  hashtags = find_hashtag(tweet)\n",
        "  for hashtag in hashtags:\n",
        "    split = split_hashtag(hashtag)\n",
        "    tweet = tweet.replace(hashtag, split)\n",
        "  tweet = tweet.replace('#', '')\n",
        "  return tweet\n",
        "\n",
        "def find_hashtag(tweet):\n",
        "  hashtags = re.findall(r\"#(\\w+)\", tweet)\n",
        "  return hashtags\n",
        "\n",
        "def split_hashtag(hashtag):\n",
        "  fo = re.compile(r'#[A-Z]{2,}(?![a-z])|[A-Z][a-z]+')\n",
        "  fi = fo.findall(hashtag)\n",
        "  result = ''\n",
        "  for var in fi:\n",
        "    result += var + ' '\n",
        "  return result\n",
        "\n",
        "# TRAINING DATA (FOR ALL THE SUBTASKS)\n",
        "def import_data(train_file, test_file_a, test_labels_a):\n",
        "  # IMPORT DATASET\n",
        "  data = pd.read_csv(train_file, sep='\\t', header=0)\n",
        "  data = data[['id','tweet', 'subtask_a', 'subtask_b', 'subtask_c']]\n",
        "\n",
        "  # DATA PREPROCESSING\n",
        "  data_preprocessing(data)\n",
        "\n",
        "  # REAL TEST DATASET\n",
        "  data_test = pd.read_csv(test_file, sep='\\t', header=0)\n",
        "\n",
        "  # DATA PREPROCESSING\n",
        "  # split hashtags into words\n",
        "\n",
        "  for i, j in data_test.iterrows():\n",
        "    data_test.at[i,'tweet'] = replace_hashtag(j['tweet'])\n",
        "  data_test['tweet'] = data_test['tweet'].apply(lambda x: x.lower()) # lowercase\n",
        "  data_test['tweet'] = data_test['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove special characters\n",
        "  data_test['tweet'] = data_test['tweet'].str.replace('user','') # remove 'user' tokens\n",
        "  data_test['tweet'] = data_test['tweet'].str.replace('url','') # remove 'url' tokens\n",
        "  labels_test = pd.read_csv(test_labels, sep=',', header=0)\n",
        "  labels_test = labels_test[['id', subtask_name]]\n",
        "  data_test = pd.merge(data_test, labels_test, on='id')\n",
        "  Y = pd.get_dummies(data[subtask_name]).values\n",
        "\n",
        "  # TOKENIZER\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "  tokenizer.fit_on_texts(data['tweet'].values)\n",
        "  X = tokenizer.texts_to_sequences(data['tweet'].values)\n",
        "  X = pad_sequences(X, maxlen=maxlen)\n",
        "\n",
        "# DATA PREPROCESSING STEPS\n",
        "\n",
        "def data_preprocessing(data):\n",
        "  for i, j in data.iterrows():\n",
        "    data.at[i,'tweet'] = replace_hashtag(j['tweet']) # split hashtags into words\n",
        "  data['tweet'] = data['tweet'].apply(lambda x: x.lower()) # lowercase\n",
        "  data['tweet'] = data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove special characters\n",
        "  data['tweet'] = data['tweet'].str.replace('user','') # remove 'user' tokens\n",
        "  data['tweet'] = data['tweet'].str.replace('url','') # remove 'url' tokens\n",
        "\n",
        "  # Testing with validation dataset\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
        "\n",
        "  # Testing with original test dataset\n",
        "  X_test_real = tokenizer.texts_to_sequences(data_test['tweet'].values)\n",
        "  X_test_real = pad_sequences(X_test_real, maxlen=maxlen)\n",
        "  Y_test_real = pd.get_dummies(data_test[subtask_name]).values\n",
        "  \n",
        "  return X_train, X_test, Y_train, Y_test, X_test_real, Y_test_real, tokenizer\n",
        "# SPLIT HASHTAG INTO WORDS UTIL FUNCTIONS\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1wekzPY8HkI"
      },
      "source": [
        "\n",
        "def pretrained_embeddings(tokeniser):\n",
        "  embedding_file = open('drive/My Drive/glove.twitter.27B.100d.txt.gz', encoding = 'utf-8')\n",
        "\n",
        "  embeddings_index = {}\n",
        "  \n",
        "  for line in embedding_file:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  embedding_file.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  embedding_matrix = np.zeros((max_fatures, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if i < max_fatures:\n",
        "          if embedding_vector is not None:\n",
        "              # Words not found in embedding index will be all-zeros.\n",
        "             embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3kSzpiuMOn"
      },
      "source": [
        "def prediction_and_results(model, X_test, Y_test, X_test_real, Y_test_real):\n",
        "\n",
        "  # PREDICTION USING VALIDATION DATASET\n",
        "  print('Model:', model_name)\n",
        "  print('Model prediction with validation dataset:')\n",
        "  \n",
        "  # Prediction for class Model\n",
        "  Y_pred = model.predict(X_test,batch_size = batch_size)\n",
        "  Y_pred = np.argmax(Y_pred,axis=1)\n",
        "  df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': Y_pred})\n",
        "  df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "  print(\"Confusion matrix:\", confusion_matrix(df_test.true, df_test.pred))\n",
        "  print(classification_report(df_test.true, df_test.pred, digits=4))\n",
        "\n",
        "  # PREDICTION USING REAL TEST DATASET\n",
        "  print('Model:', model_name)\n",
        "  print('Model prediction with test dataset:')\n",
        "\n",
        "  # Prediction for class Model\n",
        "  Y_pred_real = model.predict(X_test_real,batch_size = batch_size)\n",
        "  Y_pred_real = np.argmax(Y_pred_real,axis=1)\n",
        "  df_test_real = pd.DataFrame({'true': Y_test_real.tolist(), 'pred':Y_pred_real})\n",
        "  df_test_real['true'] = df_test_real['true'].apply(lambda x: np.argmax(x))\n",
        "  print(\"Confusion matrix\", confusion_matrix(df_test_real.true, df_test_real.pred))\n",
        "  \n",
        "  # PREDICTION FOR ONE MODEL\n",
        "  print(classification_report(df_test_real.true, df_test_real.pred, digits=4))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0RncUJsuNhl"
      },
      "source": [
        "def prediction_and_results_ensemble(ensemble_model, X_test, Y_test, X_test_real, Y_test_real):\n",
        "\n",
        "\n",
        "  y_combine = [model.predict(X_test, batch_size = batch_size) for model in ensemble_model]\n",
        "  y_combine = np.array(y_combine)\n",
        "\n",
        "  # sum across ensembles\n",
        "  summed = np.sum(y_combine, axis=0)\n",
        "\n",
        "  # argmax across classes\n",
        "  Y_pred = np.argmax(summed, axis=1)\n",
        "  df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': Y_pred})\n",
        "  df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "  print(\"Confusion matrix:\", confusion_matrix(df_test.true, df_test.pred))\n",
        "  print(classification_report(df_test.true, df_test.pred, digits=4))\n",
        "\n",
        "  # PREDICTION USING REAL TEST DATASET\n",
        "  y_combine = [model.predict(X_test_real, batch_size = batch_size) for model in ensemble_model]\n",
        "  y_combine = np.array(y_combine)\n",
        "\n",
        "  # sum across ensembles\n",
        "  summed = np.sum(y_combine, axis=0)\n",
        "  \n",
        "  # argmax across classes\n",
        "  Y_pred_real = np.argmax(summed, axis=1)\n",
        "  df_test_real = pd.DataFrame({'true': Y_test_real.tolist(), 'pred':Y_pred_real})\n",
        "  df_test_real['true'] = df_test_real['true'].apply(lambda x: np.argmax(x))\n",
        "  print(\"Confusion matrix\", confusion_matrix(df_test_real.true, df_test_real.pred))\n",
        "  print(classification_report(df_test_real.true, df_test_real.pred, digits=4))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDfPSWTeSZCH"
      },
      "source": [
        "def initialize_CNN_GMP(output):\n",
        "  model_name = 'CNN + GLOBAL MAX POOLING'\n",
        "  model = Sequential()\n",
        "  #model.add(Embedding(max_fatures, embedding_dim, input_length = maxlen, trainable = False)) #without pretrained embeddings\n",
        "  model.add(Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)) #with pretr\n",
        "  model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  # CNN + GLOBAL MAX POOLING\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(output, activation='softmax'))\n",
        "  return model, model_name"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnh272lQDIc"
      },
      "source": [
        "def initialize_CNN_3(output):\n",
        "  model_name = 'CNN'\n",
        "  embedding_dim = 200\n",
        "  i = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
        "  x = Embedding(max_fatures, embedding_dim ,weights = [embedding_matrix], input_length=maxlen, trainable=True)(i)\n",
        "  x = Dropout(0.4)(x)\n",
        "  def get_conv_pool(x_input, max_len, sufix, n_grams=[2,3,4], feature_maps=256):\n",
        "        branches = []\n",
        "        for n in n_grams:\n",
        "            branch = Conv1D(filters=feature_maps, kernel_size=n, activation='relu', name='Conv_'+sufix+'_'+str(n))(x_input)\n",
        "            branch = MaxPooling1D(pool_size=max_len-n+1, strides=None, padding='valid', name='MaxPooling_'+sufix+'_'+str(n))(bran\n",
        "            branch = Flatten(name='Flatten_'+sufix+'_'+str(n))(branch)\n",
        "            branches.append(branch)\n",
        "        return branches\n",
        "  branches = get_conv_pool(x, maxlen, 'dynamic')\n",
        "  z = concatenate(branches, axis=-1)\n",
        "  z1 = Dropout(0.3)(z)\n",
        "  z2 = Dense(256, activation='relu')(z1)\n",
        "  o = Dense(2, activation='softmax')(z2)\n",
        "  # CNN (3 filters)\n",
        "  model = Model(inputs=i, outputs=o)\n",
        "  return model, model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRklEiDoQG5L"
      },
      "source": [
        "# SUBTASK A\n",
        "# IMPORT DATA\n",
        "print('IMPORT DATA...')\n",
        "X_train, X_test_a, Y_train_a, Y_test_a, X_test_real_a, Y_test_real_a, tokenizer = import_data(train_file, test_file_a, test_labels_a)\n",
        "embedding_matrix = pretrained_embeddings(tokenizer)\n",
        "\n",
        "# For Different SUBTASKS B & C, all we need to do is change the test_file_a & test_labels_a to either\n",
        "# test_file_b or test_file_c and test_labels_b or test_labels_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzJ-sqaRQje5"
      },
      "source": [
        "# INITIALIZING OUR CNN MODELS\n",
        "\n",
        "model_name = initialize_CNN_GMP(2)\n",
        "model_name = initialize_CNN_3(2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FPw50KlQwqd"
      },
      "source": [
        "print('COMPILING MODEL')\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdiUUTDQzUL"
      },
      "source": [
        "# MODEL SCHEME\n",
        "print('PLOT THE MODEL...')\n",
        "print('Model:', model_name)\n",
        "print(model.summary())\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# SUBTASK A: FIT THE MODEL\n",
        "print('FIT THE MODEL...')\n",
        "print('Model:', model_name)\n",
        "batch_size = 128\n",
        "model.fit(X_train, Y_train_a, epochs = 10, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}