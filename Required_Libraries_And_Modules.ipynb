{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Required_Libraries_And_Modules.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRuI9C0OGwgDixVcUEcsMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YagyanshB/SemEval-Task6-CS408/blob/main/Required_Libraries_And_Modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxL7ThmnT3uR"
      },
      "source": [
        "# **CS408** - Individual Project\n",
        "# **Supervisor** - Dr. Leif Azzopardi & Dr. Amal Htait\n",
        "# **Student** - Yagyansh Bagri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMEsoZNsTajB"
      },
      "source": [
        "# **Development Environment Used** : \n",
        "---\n",
        "For the purpose of our project, we will be using **Google Colab**. **Colab** is a short form for **Collaboratory**; it is a product from **Google Research** used by leading universities and researchers across the globe.\n",
        "\n",
        "Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning and data analytics. **Google Colab** was even recommended by the project supervisor owing to it's superior computational power and the significant cost associated with the projecy. **Dr. Leif Azzopardi's** recommedation significantly propogated my interest in **Google Colab** implementing our **Machine Learning Project** within it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_1qdEblUu3K"
      },
      "source": [
        "# **Competition Description**:\n",
        "---\n",
        "**Offensive Tweets Classification** - [OffensEval 2019 - Codalab Challenge](https://sites.google.com/site/offensevalsharedtask/offenseval2019) \n",
        "\n",
        "In **OffensEval** we break down offensive content into three sub-tasks taking the type and target of offenses into account.\n",
        "\n",
        "Sub-tasks:\n",
        "\n",
        "1.   Sub-task A - Offensive language identification\n",
        "2.   Sub-task B - Automatic categorization of offense types\n",
        "3.   Sub-task C - Offense target identification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Problem Description**:\n",
        "---\n",
        "\n",
        "Offensive language is pervasive all over social media. Individuals frequently take advantage of the perceived anonymity of computer-mediated communication, using this to engage in behavior that many of them would not consider in real life. Online communities, social media platforms, and technology companies have been investing heavily in ways to cope with offensive language to prevent abusive behavior in social media.\n",
        "\n",
        "One of the most effective strategies for tackling this problem is to use computational methods to identify offense, aggression, and hate speech in user-generated content like posts, comments or tweets. For the purpose of this exploratory venture, we would only be focussing on **Tweets**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9m_PEmn1hL3"
      },
      "source": [
        "# **Identifying & Categorising Offensive Language in Social Media** \n",
        "# **(A Machine Learning Approach)**: \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The steps that we have implemented from Uploading the Dataset, Preprocessing the Dataset, Implementing Baselines, Fine Tuning Parameters and providing results have been showcased below. Additionally the social media platform that we have investigated into is **Twitter Inc.**\n",
        "\n",
        "In order to run the code in the right manner for optimal results with least amount of effort, kindly follow the steps as mentioned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZI3cz-b2SpS"
      },
      "source": [
        "#Libraries & Modules Imported:\n",
        "---\n",
        "Before we can initiate our pre-processing steps and executionable features on our dataset; it is imperative to load all the necessary libraries and modules for the right functioning of our program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTbSdrfC3YK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac358a7c-4ee0-4705-b445-f50007ace7c9"
      },
      "source": [
        "# From utils we begin our process.\n",
        "# Importing the necessary libraries and modules\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import re \n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from os import mkdir\n",
        "from os.path import join, isfile, isdir, exists\n",
        "import pickle \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "# We install and import the bcolz library for better Input & Output operational needs\n",
        "!pip install bcolz\n",
        "import bcolz\n",
        "\n",
        "#from pattern.en import spelling\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Since the Keras library wasn't installed within our version of Google Colab, we download it using the \n",
        "# below line of code.\n",
        "\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "# We now import the majorly utlisied modules for our machine learning project\n",
        "# using the below lines of code.\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense \n",
        "from keras.layers import SimpleRNN \n",
        "from keras.layers import Embedding\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.19.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2647509 sha256=6e7375ce754705be40599e6f8a8d27f21c057a6a96df7181bc74fd5949e6541f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}